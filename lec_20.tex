\lecture{20}{26/11/2024}{Chevelley basis}

\begin{proof}
    All $\lambda \in \mathfrak{h}^{*}_\R$ can be written as 
    \begin{align}
        \lambda = \sum_{i}^{} c_i \alpha_{\left( i \right) }
    ,\end{align}
    for $c_{i} \in \R$. Linear independence states that $\lambda = 0 \iff c_{i} = 0$, $\forall i$. Assume $\exists c_{i} \neq 0$. Let $J_{\pm} = \{i  \mid c_{i} \gtrless 0\} $ and define
    \begin{align}
        \lambda_{+} = \sum_{j \in J_+}^{}  c_{i} \alpha_{\left( i \right) }, && \lambda_- = -\sum_{j \in J_-}^{} c_j \alpha_{\left( j \right) } =  \sum_{j \in J_-}^{} b_j \alpha_{\left( j \right) }
    ,\end{align}
    with $b_j = - c_j > 0$. Then one can write
    \begin{align}
        \lambda = \lambda_+ - \lambda_- = \sum_{j \in J_+}^{}  c_i \alpha_{\left( i \right) } - \sum_{j \in J_-}^{}  b_{j} \alpha_{\left( j \right) }
    .\end{align}
    Then observe that the inner product
    \begin{align}
        \left( \lambda, \lambda \right) = \left( \lambda_+, \lambda_+ \right) + \left( \lambda_-, \lambda_- \right) - 2\left( \lambda_+, \lambda_- \right) > -2 \left( \lambda_+, \lambda_- \right) 
    ,\end{align}
    since at least one of $\left( \lambda_+, \lambda_+ \right)$ and $\left( \lambda_- , \lambda_- \right) $ are greater than 0. Further 
    \begin{align}
        \left( \lambda_+, \lambda_- \right) = \sum_{i \in J_+}^{}  \sum_{j \in J_-}^{}  c_i b_j \left( \alpha_{\left( i \right) }, \alpha_{\left( j \right) } \right) \leq 0
    ,\end{align}
    as $\left( \alpha_{\left( i \right) }, \alpha_{\left( j \right) } \right) \leq 0$ for distinct simple roots and $c_i b_j > 0$ by construction. Therefore $\left( \lambda, \lambda \right) > 0$ and thus the simple roots are linearly independent.
\end{proof}

There are $r = \dim \mathfrak{h}^{*}_\R$ simple roots, $\left| \Phi_S \right| = r$, as they minimally (i.e. linearly independently) span $\mathfrak{h}^{*}_\R$.

\subsection{Classification}

As the simple roots form a basis for $\mathfrak{h}^{*}_\R$ we want extend this to write a basis for $\mathfrak{g}$ using them as well. This is called a \textbf{Chevelley basis}.

\begin{definition}
    Define the $r \times  r$ \textbf{Cartan matrix} $A$ such that
    \begin{align}
        A_{ji} = \frac{2 \left( \alpha_{\left( j \right) }, \alpha_{\left( i \right) } \right) }{\left( \alpha_{\left( i \right) }, \alpha_{\left( i \right) } \right) }
    ,\end{align}
    which is notably not symmetric. However, $A_{ji}$ are integers due to the quantization condition and in fact, $A_{ji} \in \Z_{\leq 0}$ for $i \neq j$. For $i = j$, $A_{ji} = 2$.
\end{definition}

We had commutation relations for $h_{\alpha}$ and $e_{\beta}$. For $\alpha_{\left( i \right) }, \alpha_{\left( j \right) } \in \Phi_S$, 
\begin{align}
    \left[ h_{\alpha_{\left( i \right) }}, h_{\alpha_{\left( j \right) }} \right]  &= 0 \\
    \left[ h_{\alpha_{\left( i \right) }}, e_{\pm \alpha_{\left( j \right) }} \right] &= \pm A_{ji} e_{\pm \alpha_{\left( i \right)}  }, \\
    \left[ e_{\alpha_{\left( i \right) }}, e_{\alpha_{\left( j \right) }} \right] &= \delta_{ij} h_{\alpha_{\left( i \right) }}
.\end{align}

One can then see the relation
\begin{align}
    \left[ e_{\alpha_{\left( i \right) }}, e_{\alpha_{\left( j \right) }} \right] = \ad_{e_{\alpha_{\left( i \right) }}} \left( e_{\alpha_{\left( j \right) }} \right) \propto e_{\alpha_{\left( i \right) } + e_{\alpha_{\left( j \right)}  }}
,\end{align}
as long as $\alpha_{\left( i \right) } + \alpha_{\left( j \right) } \in \Phi$. In this case, $\alpha_{\left( i \right) } + \alpha_{\left(j  \right) }$ is part of a root string which we can write as $n \alpha_{\left( i \right) } + \alpha_{\left( j \right)}$ and the string has length $\ell = 1 - A_{ji}$. For $\ell$ applications of $\ad$, as the string ends we have
\begin{align}
    \left( \ad e_{\alpha_{\left( i \right) }} \right)^{1 - A_{ji}} \left( e_{\alpha_{\left( j \right) }} \right)  = 0
.\end{align}

This is the \textbf{Serre relation}.

The Cartan matrix is quite heavily constrained by the algebra. In fact, any finite dimensional, complex, Lie algebra is \textit{uniquely determined} by its Cartan matrix.

As such, the entries are of particular interest and we can identify general properties:
\begin{enumerate}[label=\roman*)]
    \item $\forall i$ fixed, $A_{i i} = 2$ on the diagonal.
    \item $A_{ji} = 0 \iff A_{ij} = 0$ by symmetry of the inner product.
    \item $A_{ji} \in \Z_{\leq 0}$ $\forall i \neq j$ from $\left( \alpha_{\left( i \right) }, \alpha_{\left( j \right) } \right) \leq 0$.
    \item $\det A > 0$.
        \begin{proof}
            Write $A = \kappa^{-1} D$ where $D^{j}_k = \frac{2}{\left( \alpha_{\left( j \right) }, \alpha_{\left( j \right) } \right) } \delta^{j}_k$ with no summation which implies $\det D > 0$. Recall $\left( \lambda, \lambda \right) = \lambda_i \left( \kappa^{-1} \right)^{ij} \lambda_j > 0$ and thus $\kappa^{-1}$ is positive definite and thus $\det A = \det \kappa^{-1} \det D > 0$ as desired.
        \end{proof}
\end{enumerate}

\begin{proposition}
    For simple Lie, algebras, $A$ is irreducible, i.e. cannot be made block triangular by a permutation transformation, where $P$ is a permutation matrix. Otherwise $\mathfrak{g}$ is semisimple.
\end{proposition}

\begin{claim}
    $A_{ij} A_{ji} \in \{0,1,2,3,\} $ for $i \neq j$ without summation.
\end{claim}

\begin{proof}
    Recall
    \begin{align}
        \left( \alpha_{\left( i \right) }, \alpha_{\left( j \right) } \right) = \left| \alpha_{\left( i \right) } \right| \left| \alpha_{\left( j \right) } \right| \cos \phi_{ij}
    ,\end{align}
    which implies
    \begin{align}
        A_{ij} A_{ji} = 4 \cos^2 \phi_{ij} \in \left[ 0,4 \right]  
    .\end{align}
    If $\phi = \{0, \pi, \cdots, n \pi\}$, then $\alpha_{\left( i \right) } = \pm \alpha_{\left( j \right) }$ which is not possible for simple roots and thus $A_{ij} A_{ji} \in \{0,1,2,3\}$.
\end{proof}

This implies for simple complex Lie algebras with rank $r = 2$ that one has
\begin{align}
    A = \mqty( 2 & -m \\ -n & 2)
,\end{align}
with $m,n \in \Z_{\geq 0}$ and $\det A = 4 - m n > 0$. This implies we have
\begin{align}
    \left( m,n \right) = \left\{ \left( 1,1 \right) , \left( 1,2 \right) , \left( 2,1 \right) , \left( 1,3 \right) , \left( 3,1 \right)  \right\} 
.\end{align}
\begin{note}
    $\left( m,n \right) = \left( 0,0 \right) $ gives a diagonal matrix which is reducible and thus not semisimple.
\end{note}

Note that the Cartan matrix completely specifies and defines the simple Lie algebra (up to relabelling roots) and thus differences like $\left( 1,2 \right) $ and $\left( 2,1 \right) $ are trivial permutations that correspond to the same algebra.

\subsection{Dynkin diagrams}

\begin{definition}
    Given a Cartan matrix $A$, its corresponding \textbf{Dynkin diagram} is defined through the procedure as follows:
    \begin{enumerate}[label=\roman*)]
        \item Draw a node for each simple root $\alpha_{\left( i \right) }$.
        \item Join the nodes representing $\alpha_{\left( i \right) }$ and $\alpha_{\left( j \right) }$ with $\text{max}\left( \left| A_{ij} \right| , \left| A_{ji} \right|  \right) \in \{0, 1, 2, 3\} $ lines.
        \item If the number of lines is greater than 1, add an arrow pointing from the longer root $\alpha_{\left( \ell \right) }$ to the shorter root, $\alpha_{\left( s \right) }$, namely, with $\left| \alpha_{\left( \ell \right) } \right| > \left| \alpha_{\left( s \right) } \right| $
    \end{enumerate}
\end{definition}

\begin{example}
    For $r = 2$ with $\left( m,n \right) = \left( 1,1 \right) $ we have
    \begin{align}
        A = \mqty( 2 & -1 \\ -1 & 2 )
    .\end{align}
\end{example}

This has Dynkin diagram \dynkin A2.

As $A_{ij} = A_{ji} = -1$ and $A_{ji} = 2 \frac{\left( \alpha_{\left( i \right) }, \alpha_{\left( j \right) } \right) }{\left( \alpha_{\left( i \right) }, \alpha_{\left( i \right) } \right) }$, these imply $\left| \alpha_{\left( 1 \right) } \right| = \left| \alpha_{\left( 2 \right) } \right| $.

Further as $A_{ij} A_{ji} = 4 \cos^2 \phi_{ij} = 1 \implies  \cos \phi_{ij} = \pm \frac{1}{2} \implies \phi = \pm \frac{2\pi}{3}$.

Let $\alpha_{\left( 1 \right) } = \left( 1,0 \right) $. Then we can plot $\alpha_{\left( 1 \right) }$ and $\alpha_{\left( 2 \right) }$ with
% plot

One can add $-\alpha_{\left( 1 \right) }$, $-\alpha_{\left( 2 \right) }$ and $\pm \left( \alpha_{\left( 1 \right)}  + \alpha_{\left( 2 \right)}   \right) $. Thus we have six roots and two vectors in the Cartan subalgebra. Thus $\mathfrak{g}$ has dimension 8. We will see that this is $\mathfrak{su}\left( 3 \right) $

If we take $\left( m,n \right) = \left( 2,1 \right) $ then
\begin{align}
    A = \mqty( 2 & -2 \\ -1 & 2)
.\end{align}

This has Dynkin diagram \dynkin B2 where the arrow is pointing towards $\alpha_{\left( 2 \right) }$ as $\alpha_{\left( 1 \right) }$ is larger (as we will show).

Observe as before that
\begin{align}
    \left| \alpha_{\left( 1 \right) } \right| = \sqrt{2} \left| \alpha_{\left( 2 \right) } \right| 
,\end{align}
and $\phi_{12} = \pm \frac{3\pi}{4}$.

Say $\alpha_{\left( 2 \right) } = \left( 1,0 \right) $ and $\alpha_{\left( 1 \right) } = \left( -1,1 \right) $ to enforce the relative length and angle. We can as before find 8 roots in total.

Lastly, we look at
\begin{align}
    A = \mqty( 2 & -3 \\ - 1 & 2)
,\end{align}
 and see it has Dynkin diagram \dynkin G2.

 \begin{note}
     If one took $m = n = 0$, then
     \begin{align}
         A = \mqty( 2 & 0 \\ 0 & 2)
     ,\end{align}
     is reducible and has disconnected Dynkin diagram \dynkin A1 \dynkin A1.
 \end{note}

 \begin{definition}
     A Dynkin diagram is \textbf{admissible} if the corresponding matrix satisfies the constraints of a Cartan matrix. Equivalently if the diagram is connected and corresponds to a system of linearly independent unit vectors $\hat{\alpha}_{\left( i \right) } = \frac{\alpha_{\left( i \right) }}{\left| \alpha_{\left( i \right) } \right| }$ such that the angles $\phi_{ij}$ between any $\hat{\alpha}_{\left( i \right) }$ and $\hat{\alpha}_{\left( j \right) }$ are $\pm \{\frac{\pi}{2}, \frac{2\pi}{3}, \frac{3\pi}{4}, \frac{5\pi}{6}\} $.
 \end{definition}

 Thus we have an equivalence between Cartan matrix constraints and Dynkin diagram constraints.
 \begin{enumerate}[label=\roman*)]
     \item The diagram must be connected as a disconnected diagram corresponds to two disconnected sets $\{\hat{\alpha}_{\left( 1 \right) } \cdots \hat{\alpha}_{\left( k \right) }\} $ and $\{\hat{\alpha}_{\left( k + 1 \right) }, \cdots, \hat{\alpha}_{\left( r \right) }\}$, $1 \leq k < r$ such that every vector in the first set is orthogonal to every vector in the second set. Thus we have orthogonal subspaces and $A$ is reducible.
     \item 
         \begin{claim}
             Given an admissible diagram, any subdiagram obtained by removing some nodes and their connections (as long as it is still connected is also admissible.
         \end{claim}
         \begin{proof}
             Follows from linear independence of a set of vectors as removing elements of the set does not remove this property.
         \end{proof}
         \item
             \begin{claim}
                 There are at more $r -1$ pairs of nodes connected by liens where $r = \dim \mathfrak{h}^{*}_\R$ and is also equal to the number of nodes.
             \end{claim}
             \begin{proof}
                 If $\hat{\alpha}_{\left( i \right) }$ and $\hat{\alpha}_{\left( j \right) }$ are connected, then $\left( \hat{\alpha}_{\left( i \right) }, \hat{\alpha}_{\left( j \right) } \right) \leq \cos \frac{2\pi}{3} = -\frac{1}{2}$ and thus nonzero. Then
                 \begin{align}
                     0 < \left( \sum_{i=1}^{r}  \hat{\alpha}_{\left( i \right) }, \sum_{i}^{r}  \hat{\alpha}_{\left( i \right) } \right) = r + 2 \sum_{i \leq j}^{}  \left( \hat{\alpha}_{\left( i \right) }, \hat{\alpha}_{j} \right)   \leq r - p
                 ,\end{align}
                 where $p$ is the number of connected pairs and thus $p < r \implies p \leq r - 1$.

                \end{proof}
             \item ~
                 \begin{corollary}
                     No admissible diagram can have a cycle as a sub diagram.
                 \end{corollary}
                 \begin{proof}
                     A cycle has $p = r$ and thus as we can remove nodes without changing admissibility, no cycle can exist.
                 \end{proof}
             \item ~
                 \begin{claim}
                     No node can have more than $3$ lines attached to it.
                 \end{claim}
                 \begin{proof}
                     Proceed by contradiction. See Wingate.
                 \end{proof}
             \item Replacing a linear chain of nodes in an admissible diagram by a single node leaves an admissible diagram.
                 This rules out diagrams that would have four edges if a line between them was removed. Namely, one cannot have two nodes with three neighbours connected by a line as the line can be replaced with a point. This follows identically for double lines
                 % figs
             \item Diagrams like
                 % m = n
                 are admissible only if $\left( m - 1 \right) \left( n - 1 \right)  < 2$.
             \item A diagram with a node connected to $3$ linear chains of length $m$, $n$ and $p$ is admissible only if
                 \begin{align}
                     \frac{1}{m} + \frac{1}{n} + \frac{1}{p} > 1
                 .\end{align}
                 This rules out another class.
 \end{enumerate}


These rules lead to \textbf{Cartan's classification}.

\begin{theorem}
    Any admissible diagram is one of the following infinite families:
    \begin{align}
        A_r &: \dynkin A{} \quad \text{for $r \geq 1$, corresponding to $L \left( SU \left( r + 1 \right)  \right)_{\C}$} \\
        B_r &: \dynkin B{} \quad \text{for $r \geq 2$, corresponding to $L \left( SO \left( 2r + 1 \right)  \right)_{\C}$} \\
        C_r &: \dynkin C{}\quad \text{for $r \geq 3$ as $C_2 = B_2$, corresponding to $L \left( Sp \left( 2r  \right)  \right)_{\C}$ } \\
        D_r &: \dynkin C{}\quad \text{for $r \geq 4$ as $D_3 = A_3$, corresponding to $L \left( SO \left( 2r  \right)  \right)_{\C}$  }
    ,\end{align}
    \textbf{or} is one of the five exceptional cases: 
    \begin{align}
        E_6 &: \dynkin E6 \\
        E_7 &: \dynkin E7 \\
        E_8 &: \dynkin E8 \\
        F_4 &: \dynkin F4 \\
        G_2 &: \dynkin G2 
    .\end{align}
\end{theorem}

Our identifications tell us that $\mathfrak{sp}\left( 4 \right)_{\C} \cong \mathfrak{so}\left( 5 \right)_\C$ and $\mathfrak{so}\left( 6 \right)_\C \cong \mathfrak{su}\left( 4 \right)_{\C}$.

\subsection{Reconstruction}

Given a Dynkin diagram we can therefore completely recover the Cartan matrix. The Cartan matrix gives us the lengths and angles between simple roots and we can use root strings to construct the whole root set. Lastly, we can use our set of roots to construct the Cartan-Weyl basis and a full set of brackets from the Jacobi identity and thus recover the algebra.



